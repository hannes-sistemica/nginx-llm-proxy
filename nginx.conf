# llm-proxy nginx configuration
#
# Proxies OpenAI-compatible API requests to multiple llama-server instances
# based on the "model" field in the JSON request body.
#
# Setup:
#   1. Copy: sudo cp nginx.conf /etc/nginx/sites-enabled/llm-proxy
#   2. Edit the two paths below to match your install location
#   3. Test: sudo nginx -t
#   4. Reload: sudo nginx -s reload

# ── EDIT THESE TWO PATHS ─────────────────────────────────
lua_package_path "/opt/llm-proxy/?.lua;;";

init_by_lua_block {
    local proxy = require("llm-proxy")
    proxy.init("/opt/llm-proxy/config.json")
}
# ──────────────────────────────────────────────────────────

server {
    # Listens on all interfaces. Change to "listen 127.0.0.1:4000;" for
    # localhost-only access, or use firewall rules for network-exposed setups.
    listen 4000;
    server_name _;

    client_max_body_size 64m;
    lua_need_request_body off;
    client_body_buffer_size 2m;

    # ── Health check (no auth) ────────────────────────────
    location = /health {
        default_type application/json;
        return 200 '{"status":"ok"}';
    }

    # ── List models (OpenAI-compatible) ───────────────────
    location = /v1/models {
        default_type application/json;
        content_by_lua_block {
            local proxy = require("llm-proxy")
            proxy.check_auth()
            proxy.models()
        }
    }

    # ── Reload config (localhost only) ────────────────────
    location = /admin/reload {
        allow 127.0.0.1;
        allow ::1;
        deny all;
        content_by_lua_block {
            local proxy = require("llm-proxy")
            local ok, err = proxy.reload()
            if ok then
                ngx.say('{"status":"reloaded"}')
            else
                ngx.status = 500
                ngx.say('{"status":"error","message":"' .. (err or "unknown") .. '"}')
            end
        }
    }

    # ── All /v1/* API endpoints ───────────────────────────
    location /v1/ {
        set $backend "";

        access_by_lua_block {
            local proxy = require("llm-proxy")
            proxy.check_auth()
            proxy.route()
        }

        proxy_pass $backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_http_version 1.1;

        # SSE streaming
        proxy_set_header Connection "";
        proxy_buffering off;
        proxy_cache off;
        chunked_transfer_encoding on;

        # Long timeouts for slow model responses
        proxy_connect_timeout 10s;
        proxy_read_timeout 600s;
        proxy_send_timeout 600s;
    }

    # ── Catch-all ─────────────────────────────────────────
    location / {
        default_type application/json;
        return 404 '{"error":{"message":"Not found. Use /v1/chat/completions, /v1/embeddings, or /v1/models","type":"invalid_request_error","code":"404"}}';
    }
}
