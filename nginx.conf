# llm-proxy nginx configuration
#
# Proxies OpenAI-compatible API requests to multiple llama-server instances
# based on the "model" field in the JSON request body.
#
# Setup:
#   1. Copy: sudo cp nginx.conf /etc/nginx/sites-enabled/llm-proxy
#   2. Edit the paths below to match your install location
#   3. Test: sudo nginx -t
#   4. Reload: sudo nginx -s reload

# ── EDIT THESE PATHS ───────────────────────────────
lua_package_path "/opt/llm-proxy/?.lua;;";
lua_shared_dict llm_stats 10m;

init_by_lua_block {
    local proxy = require("llm-proxy")
    proxy.init("/opt/llm-proxy/config.json", "/opt/llm-proxy/admin.html")
}

init_worker_by_lua_block {
    require("llm-proxy").init_worker()
}
# ──────────────────────────────────────────────────

server {
    # Listens on all interfaces. Change to "listen 127.0.0.1:4000;" for
    # localhost-only access, or use firewall rules for network-exposed setups.
    listen 4000;
    server_name _;

    client_max_body_size 64m;
    lua_need_request_body off;
    client_body_buffer_size 2m;

    # ── Health check (no auth) ────────────────────────
    location = /health {
        default_type application/json;
        return 200 '{"status":"ok"}';
    }

    # ── List models (OpenAI-compatible) ───────────────
    location = /v1/models {
        default_type application/json;
        content_by_lua_block {
            local proxy = require("llm-proxy")
            proxy.check_auth()
            proxy.models()
        }
    }

    # ── Admin static assets (JS libs, CSS) ───────────
    location /admin/static/ {
        allow 127.0.0.1;
        allow ::1;
        # allow 192.168.0.0/16;
        deny all;
        alias /opt/llm-proxy/static/;
        expires 7d;
        add_header Cache-Control "public, immutable";
    }

    # ── Admin UI (IP-restricted) ──────────────────────
    location = /admin {
        allow 127.0.0.1;
        allow ::1;
        # allow 192.168.0.0/16;
        deny all;
        content_by_lua_block {
            require("llm-proxy").serve_admin_html()
        }
    }

    # ── Admin API (IP-restricted + admin password) ────
    location /admin/api/ {
        allow 127.0.0.1;
        allow ::1;
        # allow 192.168.0.0/16;
        deny all;
        content_by_lua_block {
            require("llm-proxy").admin_api()
        }
    }

    # ── Reload config (localhost only, legacy) ────────
    location = /admin/reload {
        allow 127.0.0.1;
        allow ::1;
        deny all;
        content_by_lua_block {
            local proxy = require("llm-proxy")
            local ok, err = proxy.reload()
            if ok then
                ngx.say('{"status":"reloaded"}')
            else
                ngx.status = 500
                ngx.say('{"status":"error","message":"' .. (err or "unknown") .. '"}')
            end
        }
    }

    # ── All /v1/* API endpoints ───────────────────────
    location /v1/ {
        set $backend "";

        access_by_lua_block {
            local proxy = require("llm-proxy")
            proxy.check_auth()
            proxy.route()
        }

        proxy_pass $backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_http_version 1.1;

        # SSE streaming
        proxy_set_header Connection "";
        proxy_buffering off;
        proxy_cache off;
        chunked_transfer_encoding on;

        # Long timeouts for slow model responses
        proxy_connect_timeout 10s;
        proxy_read_timeout 600s;
        proxy_send_timeout 600s;

        # Capture token usage from response
        body_filter_by_lua_block {
            require("llm-proxy").capture_response()
        }

        log_by_lua_block {
            require("llm-proxy").log_usage()
        }
    }

    # ── Catch-all ─────────────────────────────────────
    location / {
        default_type application/json;
        return 404 '{"error":{"message":"Not found. Use /v1/chat/completions, /v1/embeddings, or /v1/models","type":"invalid_request_error","code":"404"}}';
    }
}
